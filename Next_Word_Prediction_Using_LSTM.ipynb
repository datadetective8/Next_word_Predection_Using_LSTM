{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-0FJgN_FNII"
      },
      "source": [
        "The Project is going to predict the next word like When we search on we will be getting some suggestios\n",
        "in this projects stepts like\n",
        "1.   Data Collection\n",
        "2.   EDA\n",
        "1.   Preprocessing\n",
        "1.   Vectorization\n",
        "1.   Padding\n",
        "2.   LSTM\n",
        "2.   Inference\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ncp4xjjsHIpc"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.layers import Embedding,LSTM,Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import pickle\n",
        "import numpy as np\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AC7WLxGrMeG_"
      },
      "outputs": [],
      "source": [
        "files = open(\"/content/NLP_Data1.txt\",'r',encoding=\"utf8\")\n",
        "\n",
        "lines = []\n",
        "\n",
        "for i in files:\n",
        "  lines.append(i)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XkDHzw6wNTWk"
      },
      "outputs": [],
      "source": [
        "\n",
        "lines[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NJ8LkDIENUkI"
      },
      "outputs": [],
      "source": [
        "data = \"\"\n",
        "for i in lines:\n",
        "  data = \" \".join(lines)\n",
        "data = data.replace('\\n','').replace('\\r','').replace('\\ufeff','')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NCYpEl5nODXG"
      },
      "outputs": [],
      "source": [
        "data[:400]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-u5ffQrPw86"
      },
      "outputs": [],
      "source": [
        "# Removing the Punctauation\n",
        "import string\n",
        "translator = str.maketrans(string.punctuation,' '*len(string.punctuation))\n",
        "new_data = data.translate(translator)\n",
        "new_data[:500]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pZNqw0fjP0xt"
      },
      "outputs": [],
      "source": [
        "new = []\n",
        "for i in data.split():\n",
        "  if i not in new:\n",
        "    new.append(i)\n",
        "data = \" \".join(new)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RIOLrxZDRDbE"
      },
      "outputs": [],
      "source": [
        "data[:400]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p-7HvufWRGPP"
      },
      "outputs": [],
      "source": [
        "tokenizer = Tokenizer()\n",
        "\n",
        "tokenizer.fit_on_texts([data])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hUz-g4zERc53"
      },
      "outputs": [],
      "source": [
        "pickle.dump(tokenizer,open(\"tokenizer.pkl\",\"wb\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dKymKG0JRw6E"
      },
      "outputs": [],
      "source": [
        "sequence_data = tokenizer.texts_to_sequences([data])[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jo7L1JfcSI-O"
      },
      "outputs": [],
      "source": [
        "sequence_data[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X9cX9IUPSibH"
      },
      "outputs": [],
      "source": [
        "vacab_size = len(tokenizer.word_index)+1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xkPItyr_S2AH"
      },
      "outputs": [],
      "source": [
        "vacab_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KRlvXxHmS5DZ"
      },
      "outputs": [],
      "source": [
        "sequences = []\n",
        "for i in range(1,len(sequence_data)):\n",
        "  words = sequence_data[i-1:i+1]\n",
        "  sequences.append(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94jkQTd4TiUV"
      },
      "outputs": [],
      "source": [
        "len(sequences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oKrMXLSsTmNU"
      },
      "outputs": [],
      "source": [
        "sequences = np.array(sequences)\n",
        "sequences[:120]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-tMrtY-UTxPx"
      },
      "outputs": [],
      "source": [
        "X = []\n",
        "Y = []\n",
        "for i in sequences:\n",
        "  X.append(i[0])\n",
        "  Y.append(i[0])\n",
        "X = np.array(X)\n",
        "Y = np.array(Y)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4wOkdClzUart"
      },
      "outputs": [],
      "source": [
        "X"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WHdxhsdIUcB_"
      },
      "outputs": [],
      "source": [
        "Y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F_Xb_qXDUcfK"
      },
      "outputs": [],
      "source": [
        "y = to_categorical(Y,num_classes=vacab_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wqqSWOhiUxdt"
      },
      "outputs": [],
      "source": [
        "y[:10]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zvb_lu-qU6Ak"
      },
      "outputs": [],
      "source": [
        "model =  Sequential()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IkU_SaBlVO8P"
      },
      "outputs": [],
      "source": [
        "model.add(Embedding(vacab_size, 10 ,input_length = 1))\n",
        "model.add(LSTM(units = 1024,return_sequences = True))\n",
        "model.add(LSTM(units = 1024,return_sequences = True))\n",
        "model.add(LSTM(units = 1024))\n",
        "model.add(Dense(1024,activation =\"relu\"))\n",
        "model.add(Dense(vacab_size, activation = \"softmax\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sax-zLpgdqen"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QdmyNSL5368T"
      },
      "outputs": [],
      "source": [
        "\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau,TensorBoard\n",
        "\n",
        "checkpoint = ModelCheckpoint('model_nwp.h5',monitor = 'loss',verbose = 1,save_best_only = True, mode = 'auto')\n",
        "reduces = ReduceLROnPlateau(monitor = 'loss',factor = 0.2,patience = 3,min_lr = 0.0001,verbose = 1)\n",
        "logdir = 'logsnw'\n",
        "tensorboard_vis = TensorBoard(log_dir = logdir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5YnHzCFR7Qhx"
      },
      "outputs": [],
      "source": [
        "\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "model.compile(optimizer = Adam(lr = 0.001),loss = \"categorical_crossentropy\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lHiF47T97UQ5"
      },
      "outputs": [],
      "source": [
        "model.fit(X,y, epochs = 75,batch_size=64,callbacks = [checkpoint,reduces,tensorboard_vis])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KiQpfr9_81Ix"
      },
      "outputs": [],
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir=\"./logsnw\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dDhAEjvcAC2B"
      },
      "outputs": [],
      "source": [
        "def predictive_text_next(text):\n",
        "  sequence = tokenizer.texts_to_sequences([text])[0]\n",
        "  sequence = np.array(sequence)\n",
        "  preds = model.predict(sequence)\n",
        "  p_class = np.argmax(preds)\n",
        "  p_word = tokenizer.index_word[p_class]\n",
        "\n",
        "  return p_word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ys9wH-2lAC6S"
      },
      "outputs": [],
      "source": [
        "text = input(\"Enter Text : \")\n",
        "text = text.split(' ')\n",
        "text = text[-1]\n",
        "text = ''.join(text)\n",
        "predictive_text_next(text)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}